begin_unit
comment|'# Copyright (c) 2010-2015 OpenStack Foundation'
nl|'\n'
comment|'#'
nl|'\n'
comment|'# Licensed under the Apache License, Version 2.0 (the "License");'
nl|'\n'
comment|'# you may not use this file except in compliance with the License.'
nl|'\n'
comment|'# You may obtain a copy of the License at'
nl|'\n'
comment|'#'
nl|'\n'
comment|'#    http://www.apache.org/licenses/LICENSE-2.0'
nl|'\n'
comment|'#'
nl|'\n'
comment|'# Unless required by applicable law or agreed to in writing, software'
nl|'\n'
comment|'# distributed under the License is distributed on an "AS IS" BASIS,'
nl|'\n'
comment|'# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or'
nl|'\n'
comment|'# implied.'
nl|'\n'
comment|'# See the License for the specific language governing permissions and'
nl|'\n'
comment|'# limitations under the License.'
nl|'\n'
nl|'\n'
name|'import'
name|'os'
newline|'\n'
name|'from'
name|'os'
op|'.'
name|'path'
name|'import'
name|'join'
newline|'\n'
name|'import'
name|'random'
newline|'\n'
name|'import'
name|'time'
newline|'\n'
name|'import'
name|'itertools'
newline|'\n'
name|'from'
name|'collections'
name|'import'
name|'defaultdict'
newline|'\n'
name|'import'
name|'six'
newline|'\n'
name|'import'
name|'six'
op|'.'
name|'moves'
op|'.'
name|'cPickle'
name|'as'
name|'pickle'
newline|'\n'
name|'import'
name|'shutil'
newline|'\n'
nl|'\n'
name|'from'
name|'eventlet'
name|'import'
op|'('
name|'GreenPile'
op|','
name|'GreenPool'
op|','
name|'Timeout'
op|','
name|'sleep'
op|','
name|'hubs'
op|','
name|'tpool'
op|','
nl|'\n'
name|'spawn'
op|')'
newline|'\n'
name|'from'
name|'eventlet'
op|'.'
name|'support'
op|'.'
name|'greenlets'
name|'import'
name|'GreenletExit'
newline|'\n'
nl|'\n'
name|'from'
name|'swift'
name|'import'
name|'gettext_'
name|'as'
name|'_'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'utils'
name|'import'
op|'('
nl|'\n'
name|'whataremyips'
op|','
name|'unlink_older_than'
op|','
name|'compute_eta'
op|','
name|'get_logger'
op|','
nl|'\n'
name|'dump_recon_cache'
op|','
name|'mkdirs'
op|','
name|'config_true_value'
op|','
name|'list_from_csv'
op|','
name|'get_hub'
op|','
nl|'\n'
name|'tpool_reraise'
op|','
name|'GreenAsyncPile'
op|','
name|'Timestamp'
op|','
name|'remove_file'
op|')'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'header_key_dict'
name|'import'
name|'HeaderKeyDict'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'bufferedhttp'
name|'import'
name|'http_connect'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'daemon'
name|'import'
name|'Daemon'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'ring'
op|'.'
name|'utils'
name|'import'
name|'is_local_device'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'obj'
op|'.'
name|'ssync_sender'
name|'import'
name|'Sender'
name|'as'
name|'ssync_sender'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'http'
name|'import'
name|'HTTP_OK'
op|','
name|'HTTP_NOT_FOUND'
op|','
name|'HTTP_INSUFFICIENT_STORAGE'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'obj'
op|'.'
name|'diskfile'
name|'import'
name|'DiskFileRouter'
op|','
name|'get_data_dir'
op|','
name|'get_tmp_dir'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'storage_policy'
name|'import'
name|'POLICIES'
op|','
name|'EC_POLICY'
newline|'\n'
name|'from'
name|'swift'
op|'.'
name|'common'
op|'.'
name|'exceptions'
name|'import'
name|'ConnectionTimeout'
op|','
name|'DiskFileError'
op|','
name|'SuffixSyncError'
newline|'\n'
nl|'\n'
name|'SYNC'
op|','
name|'REVERT'
op|'='
op|'('
string|"'sync_only'"
op|','
string|"'sync_revert'"
op|')'
newline|'\n'
nl|'\n'
nl|'\n'
name|'hubs'
op|'.'
name|'use_hub'
op|'('
name|'get_hub'
op|'('
op|')'
op|')'
newline|'\n'
nl|'\n'
nl|'\n'
DECL|function|_get_partners
name|'def'
name|'_get_partners'
op|'('
name|'frag_index'
op|','
name|'part_nodes'
op|')'
op|':'
newline|'\n'
indent|'    '
string|'"""\n    Returns the left and right partners of the node whose index is\n    equal to the given frag_index.\n\n    :param frag_index: a fragment index\n    :param part_nodes: a list of primary nodes\n    :returns: [<node-to-left>, <node-to-right>]\n    """'
newline|'\n'
name|'return'
op|'['
nl|'\n'
name|'part_nodes'
op|'['
op|'('
name|'frag_index'
op|'-'
number|'1'
op|')'
op|'%'
name|'len'
op|'('
name|'part_nodes'
op|')'
op|']'
op|','
nl|'\n'
name|'part_nodes'
op|'['
op|'('
name|'frag_index'
op|'+'
number|'1'
op|')'
op|'%'
name|'len'
op|'('
name|'part_nodes'
op|')'
op|']'
op|','
nl|'\n'
op|']'
newline|'\n'
nl|'\n'
nl|'\n'
DECL|class|RebuildingECDiskFileStream
dedent|''
name|'class'
name|'RebuildingECDiskFileStream'
op|'('
name|'object'
op|')'
op|':'
newline|'\n'
indent|'    '
string|'"""\n    This class wraps the the reconstructed fragment archive data and\n    metadata in the DiskFile interface for ssync.\n    """'
newline|'\n'
nl|'\n'
DECL|member|__init__
name|'def'
name|'__init__'
op|'('
name|'self'
op|','
name|'datafile_metadata'
op|','
name|'frag_index'
op|','
name|'rebuilt_fragment_iter'
op|')'
op|':'
newline|'\n'
comment|'# start with metadata from a participating FA'
nl|'\n'
indent|'        '
name|'self'
op|'.'
name|'datafile_metadata'
op|'='
name|'datafile_metadata'
newline|'\n'
nl|'\n'
comment|'# the new FA is going to have the same length as others in the set'
nl|'\n'
name|'self'
op|'.'
name|'_content_length'
op|'='
name|'self'
op|'.'
name|'datafile_metadata'
op|'['
string|"'Content-Length'"
op|']'
newline|'\n'
nl|'\n'
comment|'# update the FI and delete the ETag, the obj server will'
nl|'\n'
comment|'# recalc on the other side...'
nl|'\n'
name|'self'
op|'.'
name|'datafile_metadata'
op|'['
string|"'X-Object-Sysmeta-Ec-Frag-Index'"
op|']'
op|'='
name|'frag_index'
newline|'\n'
name|'for'
name|'etag_key'
name|'in'
op|'('
string|"'ETag'"
op|','
string|"'Etag'"
op|')'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'datafile_metadata'
op|'.'
name|'pop'
op|'('
name|'etag_key'
op|','
name|'None'
op|')'
newline|'\n'
nl|'\n'
dedent|''
name|'self'
op|'.'
name|'frag_index'
op|'='
name|'frag_index'
newline|'\n'
name|'self'
op|'.'
name|'rebuilt_fragment_iter'
op|'='
name|'rebuilt_fragment_iter'
newline|'\n'
nl|'\n'
DECL|member|get_metadata
dedent|''
name|'def'
name|'get_metadata'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'return'
name|'self'
op|'.'
name|'datafile_metadata'
newline|'\n'
nl|'\n'
DECL|member|get_datafile_metadata
dedent|''
name|'def'
name|'get_datafile_metadata'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'return'
name|'self'
op|'.'
name|'datafile_metadata'
newline|'\n'
nl|'\n'
dedent|''
op|'@'
name|'property'
newline|'\n'
DECL|member|content_length
name|'def'
name|'content_length'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'return'
name|'self'
op|'.'
name|'_content_length'
newline|'\n'
nl|'\n'
DECL|member|reader
dedent|''
name|'def'
name|'reader'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'for'
name|'chunk'
name|'in'
name|'self'
op|'.'
name|'rebuilt_fragment_iter'
op|':'
newline|'\n'
indent|'            '
name|'yield'
name|'chunk'
newline|'\n'
nl|'\n'
nl|'\n'
DECL|class|ObjectReconstructor
dedent|''
dedent|''
dedent|''
name|'class'
name|'ObjectReconstructor'
op|'('
name|'Daemon'
op|')'
op|':'
newline|'\n'
indent|'    '
string|'"""\n    Reconstruct objects using erasure code.  And also rebalance EC Fragment\n    Archive objects off handoff nodes.\n\n    Encapsulates most logic and data needed by the object reconstruction\n    process. Each call to .reconstruct() performs one pass.  It\'s up to the\n    caller to do this in a loop.\n    """'
newline|'\n'
nl|'\n'
DECL|member|__init__
name|'def'
name|'__init__'
op|'('
name|'self'
op|','
name|'conf'
op|','
name|'logger'
op|'='
name|'None'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        :param conf: configuration object obtained from ConfigParser\n        :param logger: logging object\n        """'
newline|'\n'
name|'self'
op|'.'
name|'conf'
op|'='
name|'conf'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'='
name|'logger'
name|'or'
name|'get_logger'
op|'('
nl|'\n'
name|'conf'
op|','
name|'log_route'
op|'='
string|"'object-reconstructor'"
op|')'
newline|'\n'
name|'self'
op|'.'
name|'devices_dir'
op|'='
name|'conf'
op|'.'
name|'get'
op|'('
string|"'devices'"
op|','
string|"'/srv/node'"
op|')'
newline|'\n'
name|'self'
op|'.'
name|'mount_check'
op|'='
name|'config_true_value'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'mount_check'"
op|','
string|"'true'"
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'swift_dir'
op|'='
name|'conf'
op|'.'
name|'get'
op|'('
string|"'swift_dir'"
op|','
string|"'/etc/swift'"
op|')'
newline|'\n'
name|'self'
op|'.'
name|'bind_ip'
op|'='
name|'conf'
op|'.'
name|'get'
op|'('
string|"'bind_ip'"
op|','
string|"'0.0.0.0'"
op|')'
newline|'\n'
name|'self'
op|'.'
name|'servers_per_port'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'servers_per_port'"
op|','
string|"'0'"
op|')'
name|'or'
number|'0'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'port'
op|'='
name|'None'
name|'if'
name|'self'
op|'.'
name|'servers_per_port'
name|'else'
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'bind_port'"
op|','
number|'6000'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'concurrency'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'concurrency'"
op|','
number|'1'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'stats_interval'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'stats_interval'"
op|','
string|"'300'"
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'ring_check_interval'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'ring_check_interval'"
op|','
number|'15'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'next_check'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'+'
name|'self'
op|'.'
name|'ring_check_interval'
newline|'\n'
name|'self'
op|'.'
name|'reclaim_age'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'reclaim_age'"
op|','
number|'86400'
op|'*'
number|'7'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'partition_times'
op|'='
op|'['
op|']'
newline|'\n'
name|'self'
op|'.'
name|'interval'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'interval'"
op|')'
name|'or'
nl|'\n'
name|'conf'
op|'.'
name|'get'
op|'('
string|"'run_pause'"
op|')'
name|'or'
number|'30'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'http_timeout'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'http_timeout'"
op|','
number|'60'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'lockup_timeout'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'lockup_timeout'"
op|','
number|'1800'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'recon_cache_path'
op|'='
name|'conf'
op|'.'
name|'get'
op|'('
string|"'recon_cache_path'"
op|','
nl|'\n'
string|"'/var/cache/swift'"
op|')'
newline|'\n'
name|'self'
op|'.'
name|'rcache'
op|'='
name|'os'
op|'.'
name|'path'
op|'.'
name|'join'
op|'('
name|'self'
op|'.'
name|'recon_cache_path'
op|','
string|'"object.recon"'
op|')'
newline|'\n'
comment|'# defaults subject to change after beta'
nl|'\n'
name|'self'
op|'.'
name|'conn_timeout'
op|'='
name|'float'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'conn_timeout'"
op|','
number|'0.5'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'node_timeout'
op|'='
name|'float'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'node_timeout'"
op|','
number|'10'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'network_chunk_size'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'network_chunk_size'"
op|','
number|'65536'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'disk_chunk_size'
op|'='
name|'int'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'disk_chunk_size'"
op|','
number|'65536'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'headers'
op|'='
op|'{'
nl|'\n'
string|"'Content-Length'"
op|':'
string|"'0'"
op|','
nl|'\n'
string|"'user-agent'"
op|':'
string|"'obj-reconstructor %s'"
op|'%'
name|'os'
op|'.'
name|'getpid'
op|'('
op|')'
op|'}'
newline|'\n'
name|'self'
op|'.'
name|'handoffs_first'
op|'='
name|'config_true_value'
op|'('
name|'conf'
op|'.'
name|'get'
op|'('
string|"'handoffs_first'"
op|','
nl|'\n'
name|'False'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'_df_router'
op|'='
name|'DiskFileRouter'
op|'('
name|'conf'
op|','
name|'self'
op|'.'
name|'logger'
op|')'
newline|'\n'
nl|'\n'
DECL|member|load_object_ring
dedent|''
name|'def'
name|'load_object_ring'
op|'('
name|'self'
op|','
name|'policy'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Make sure the policy\'s rings are loaded.\n\n        :param policy: the StoragePolicy instance\n        :returns: appropriate ring object\n        """'
newline|'\n'
name|'policy'
op|'.'
name|'load_ring'
op|'('
name|'self'
op|'.'
name|'swift_dir'
op|')'
newline|'\n'
name|'return'
name|'policy'
op|'.'
name|'object_ring'
newline|'\n'
nl|'\n'
DECL|member|check_ring
dedent|''
name|'def'
name|'check_ring'
op|'('
name|'self'
op|','
name|'object_ring'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Check to see if the ring has been updated\n\n        :param object_ring: the ring to check\n        :returns: boolean indicating whether or not the ring has changed\n        """'
newline|'\n'
name|'if'
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'>'
name|'self'
op|'.'
name|'next_check'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'next_check'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'+'
name|'self'
op|'.'
name|'ring_check_interval'
newline|'\n'
name|'if'
name|'object_ring'
op|'.'
name|'has_changed'
op|'('
op|')'
op|':'
newline|'\n'
indent|'                '
name|'return'
name|'False'
newline|'\n'
dedent|''
dedent|''
name|'return'
name|'True'
newline|'\n'
nl|'\n'
DECL|member|_full_path
dedent|''
name|'def'
name|'_full_path'
op|'('
name|'self'
op|','
name|'node'
op|','
name|'part'
op|','
name|'path'
op|','
name|'policy'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'return'
string|"'%(replication_ip)s:%(replication_port)s'"
string|"'/%(device)s/%(part)s%(path)s '"
string|"'policy#%(policy)d frag#%(frag_index)s'"
op|'%'
op|'{'
nl|'\n'
string|"'replication_ip'"
op|':'
name|'node'
op|'['
string|"'replication_ip'"
op|']'
op|','
nl|'\n'
string|"'replication_port'"
op|':'
name|'node'
op|'['
string|"'replication_port'"
op|']'
op|','
nl|'\n'
string|"'device'"
op|':'
name|'node'
op|'['
string|"'device'"
op|']'
op|','
nl|'\n'
string|"'part'"
op|':'
name|'part'
op|','
string|"'path'"
op|':'
name|'path'
op|','
nl|'\n'
string|"'policy'"
op|':'
name|'policy'
op|','
nl|'\n'
string|"'frag_index'"
op|':'
name|'node'
op|'.'
name|'get'
op|'('
string|"'index'"
op|','
string|"'handoff'"
op|')'
op|','
nl|'\n'
op|'}'
newline|'\n'
nl|'\n'
DECL|member|_get_response
dedent|''
name|'def'
name|'_get_response'
op|'('
name|'self'
op|','
name|'node'
op|','
name|'part'
op|','
name|'path'
op|','
name|'headers'
op|','
name|'policy'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Helper method for reconstruction that GETs a single EC fragment\n        archive\n\n        :param node: the node to GET from\n        :param part: the partition\n        :param path: full path of the desired EC archive\n        :param headers: the headers to send\n        :param policy: an instance of\n                       :class:`~swift.common.storage_policy.BaseStoragePolicy`\n        :returns: response\n        """'
newline|'\n'
name|'resp'
op|'='
name|'None'
newline|'\n'
name|'try'
op|':'
newline|'\n'
indent|'            '
name|'with'
name|'ConnectionTimeout'
op|'('
name|'self'
op|'.'
name|'conn_timeout'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'conn'
op|'='
name|'http_connect'
op|'('
name|'node'
op|'['
string|"'ip'"
op|']'
op|','
name|'node'
op|'['
string|"'port'"
op|']'
op|','
name|'node'
op|'['
string|"'device'"
op|']'
op|','
nl|'\n'
name|'part'
op|','
string|"'GET'"
op|','
name|'path'
op|','
name|'headers'
op|'='
name|'headers'
op|')'
newline|'\n'
dedent|''
name|'with'
name|'Timeout'
op|'('
name|'self'
op|'.'
name|'node_timeout'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'resp'
op|'='
name|'conn'
op|'.'
name|'getresponse'
op|'('
op|')'
newline|'\n'
dedent|''
name|'if'
name|'resp'
op|'.'
name|'status'
name|'not'
name|'in'
op|'['
name|'HTTP_OK'
op|','
name|'HTTP_NOT_FOUND'
op|']'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'warning'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Invalid response %(resp)s from %(full_path)s"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'resp'"
op|':'
name|'resp'
op|'.'
name|'status'
op|','
nl|'\n'
string|"'full_path'"
op|':'
name|'self'
op|'.'
name|'_full_path'
op|'('
name|'node'
op|','
name|'part'
op|','
name|'path'
op|','
name|'policy'
op|')'
op|'}'
op|')'
newline|'\n'
name|'resp'
op|'='
name|'None'
newline|'\n'
dedent|''
name|'elif'
name|'resp'
op|'.'
name|'status'
op|'=='
name|'HTTP_NOT_FOUND'
op|':'
newline|'\n'
indent|'                '
name|'resp'
op|'='
name|'None'
newline|'\n'
dedent|''
dedent|''
name|'except'
op|'('
name|'Exception'
op|','
name|'Timeout'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Trying to GET %(full_path)s"'
op|')'
op|','
op|'{'
nl|'\n'
string|"'full_path'"
op|':'
name|'self'
op|'.'
name|'_full_path'
op|'('
name|'node'
op|','
name|'part'
op|','
name|'path'
op|','
name|'policy'
op|')'
op|'}'
op|')'
newline|'\n'
dedent|''
name|'return'
name|'resp'
newline|'\n'
nl|'\n'
DECL|member|reconstruct_fa
dedent|''
name|'def'
name|'reconstruct_fa'
op|'('
name|'self'
op|','
name|'job'
op|','
name|'node'
op|','
name|'datafile_metadata'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Reconstructs a fragment archive - this method is called from ssync\n        after a remote node responds that is missing this object - the local\n        diskfile is opened to provide metadata - but to reconstruct the\n        missing fragment archive we must connect to multiple object servers.\n\n        :param job: job from ssync_sender\n        :param node: node that we\'re rebuilding to\n        :param datafile_metadata:  the datafile metadata to attach to\n                                   the rebuilt fragment archive\n        :returns: a DiskFile like class for use by ssync\n        :raises DiskFileError: if the fragment archive cannot be reconstructed\n        """'
newline|'\n'
nl|'\n'
name|'part_nodes'
op|'='
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'object_ring'
op|'.'
name|'get_part_nodes'
op|'('
nl|'\n'
name|'job'
op|'['
string|"'partition'"
op|']'
op|')'
newline|'\n'
name|'part_nodes'
op|'.'
name|'remove'
op|'('
name|'node'
op|')'
newline|'\n'
nl|'\n'
comment|'# the fragment index we need to reconstruct is the position index'
nl|'\n'
comment|"# of the node we're rebuilding to within the primary part list"
nl|'\n'
name|'fi_to_rebuild'
op|'='
name|'node'
op|'['
string|"'index'"
op|']'
newline|'\n'
nl|'\n'
comment|'# KISS send out connection requests to all nodes, see what sticks'
nl|'\n'
name|'headers'
op|'='
name|'self'
op|'.'
name|'headers'
op|'.'
name|'copy'
op|'('
op|')'
newline|'\n'
name|'headers'
op|'['
string|"'X-Backend-Storage-Policy-Index'"
op|']'
op|'='
name|'int'
op|'('
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
newline|'\n'
name|'pile'
op|'='
name|'GreenAsyncPile'
op|'('
name|'len'
op|'('
name|'part_nodes'
op|')'
op|')'
newline|'\n'
name|'path'
op|'='
name|'datafile_metadata'
op|'['
string|"'name'"
op|']'
newline|'\n'
name|'for'
name|'node'
name|'in'
name|'part_nodes'
op|':'
newline|'\n'
indent|'            '
name|'pile'
op|'.'
name|'spawn'
op|'('
name|'self'
op|'.'
name|'_get_response'
op|','
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
nl|'\n'
name|'path'
op|','
name|'headers'
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
newline|'\n'
dedent|''
name|'responses'
op|'='
op|'['
op|']'
newline|'\n'
name|'etag'
op|'='
name|'None'
newline|'\n'
name|'for'
name|'resp'
name|'in'
name|'pile'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'not'
name|'resp'
op|':'
newline|'\n'
indent|'                '
name|'continue'
newline|'\n'
dedent|''
name|'resp'
op|'.'
name|'headers'
op|'='
name|'HeaderKeyDict'
op|'('
name|'resp'
op|'.'
name|'getheaders'
op|'('
op|')'
op|')'
newline|'\n'
name|'if'
name|'str'
op|'('
name|'fi_to_rebuild'
op|')'
op|'=='
name|'resp'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Object-Sysmeta-Ec-Frag-Index'"
op|')'
op|':'
newline|'\n'
indent|'                '
name|'continue'
newline|'\n'
dedent|''
name|'if'
name|'resp'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Object-Sysmeta-Ec-Frag-Index'"
op|')'
name|'in'
name|'set'
op|'('
nl|'\n'
name|'r'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Object-Sysmeta-Ec-Frag-Index'"
op|')'
nl|'\n'
name|'for'
name|'r'
name|'in'
name|'responses'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'continue'
newline|'\n'
dedent|''
name|'responses'
op|'.'
name|'append'
op|'('
name|'resp'
op|')'
newline|'\n'
name|'etag'
op|'='
name|'sorted'
op|'('
name|'responses'
op|','
name|'reverse'
op|'='
name|'True'
op|','
nl|'\n'
name|'key'
op|'='
name|'lambda'
name|'r'
op|':'
name|'Timestamp'
op|'('
nl|'\n'
name|'r'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Backend-Timestamp'"
op|')'
nl|'\n'
op|')'
op|')'
op|'['
number|'0'
op|']'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Object-Sysmeta-Ec-Etag'"
op|')'
newline|'\n'
name|'responses'
op|'='
op|'['
name|'r'
name|'for'
name|'r'
name|'in'
name|'responses'
name|'if'
nl|'\n'
name|'r'
op|'.'
name|'headers'
op|'.'
name|'get'
op|'('
string|"'X-Object-Sysmeta-Ec-Etag'"
op|')'
op|'=='
name|'etag'
op|']'
newline|'\n'
nl|'\n'
name|'if'
name|'len'
op|'('
name|'responses'
op|')'
op|'>='
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'ec_ndata'
op|':'
newline|'\n'
indent|'                '
name|'break'
newline|'\n'
dedent|''
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'error'
op|'('
nl|'\n'
string|"'Unable to get enough responses (%s/%s) '"
nl|'\n'
string|"'to reconstruct %s with ETag %s'"
op|'%'
op|'('
nl|'\n'
name|'len'
op|'('
name|'responses'
op|')'
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'ec_ndata'
op|','
nl|'\n'
name|'self'
op|'.'
name|'_full_path'
op|'('
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
nl|'\n'
name|'datafile_metadata'
op|'['
string|"'name'"
op|']'
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
op|','
nl|'\n'
name|'etag'
op|')'
op|')'
newline|'\n'
name|'raise'
name|'DiskFileError'
op|'('
string|"'Unable to reconstruct EC archive'"
op|')'
newline|'\n'
nl|'\n'
dedent|''
name|'rebuilt_fragment_iter'
op|'='
name|'self'
op|'.'
name|'make_rebuilt_fragment_iter'
op|'('
nl|'\n'
name|'responses'
op|'['
op|':'
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'ec_ndata'
op|']'
op|','
name|'path'
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|','
nl|'\n'
name|'fi_to_rebuild'
op|')'
newline|'\n'
name|'return'
name|'RebuildingECDiskFileStream'
op|'('
name|'datafile_metadata'
op|','
name|'fi_to_rebuild'
op|','
nl|'\n'
name|'rebuilt_fragment_iter'
op|')'
newline|'\n'
nl|'\n'
DECL|member|_reconstruct
dedent|''
name|'def'
name|'_reconstruct'
op|'('
name|'self'
op|','
name|'policy'
op|','
name|'fragment_payload'
op|','
name|'frag_index'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'return'
name|'policy'
op|'.'
name|'pyeclib_driver'
op|'.'
name|'reconstruct'
op|'('
name|'fragment_payload'
op|','
nl|'\n'
op|'['
name|'frag_index'
op|']'
op|')'
op|'['
number|'0'
op|']'
newline|'\n'
nl|'\n'
DECL|member|make_rebuilt_fragment_iter
dedent|''
name|'def'
name|'make_rebuilt_fragment_iter'
op|'('
name|'self'
op|','
name|'responses'
op|','
name|'path'
op|','
name|'policy'
op|','
name|'frag_index'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Turn a set of connections from backend object servers into a generator\n        that yields up the rebuilt fragment archive for frag_index.\n        """'
newline|'\n'
nl|'\n'
DECL|function|_get_one_fragment
name|'def'
name|'_get_one_fragment'
op|'('
name|'resp'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'buff'
op|'='
string|"''"
newline|'\n'
name|'remaining_bytes'
op|'='
name|'policy'
op|'.'
name|'fragment_size'
newline|'\n'
name|'while'
name|'remaining_bytes'
op|':'
newline|'\n'
indent|'                '
name|'chunk'
op|'='
name|'resp'
op|'.'
name|'read'
op|'('
name|'remaining_bytes'
op|')'
newline|'\n'
name|'if'
name|'not'
name|'chunk'
op|':'
newline|'\n'
indent|'                    '
name|'break'
newline|'\n'
dedent|''
name|'remaining_bytes'
op|'-='
name|'len'
op|'('
name|'chunk'
op|')'
newline|'\n'
name|'buff'
op|'+='
name|'chunk'
newline|'\n'
dedent|''
name|'return'
name|'buff'
newline|'\n'
nl|'\n'
DECL|function|fragment_payload_iter
dedent|''
name|'def'
name|'fragment_payload_iter'
op|'('
op|')'
op|':'
newline|'\n'
comment|'# We need a fragment from each connections, so best to'
nl|'\n'
comment|'# use a GreenPile to keep them ordered and in sync'
nl|'\n'
indent|'            '
name|'pile'
op|'='
name|'GreenPile'
op|'('
name|'len'
op|'('
name|'responses'
op|')'
op|')'
newline|'\n'
name|'while'
name|'True'
op|':'
newline|'\n'
indent|'                '
name|'for'
name|'resp'
name|'in'
name|'responses'
op|':'
newline|'\n'
indent|'                    '
name|'pile'
op|'.'
name|'spawn'
op|'('
name|'_get_one_fragment'
op|','
name|'resp'
op|')'
newline|'\n'
dedent|''
name|'try'
op|':'
newline|'\n'
indent|'                    '
name|'with'
name|'Timeout'
op|'('
name|'self'
op|'.'
name|'node_timeout'
op|')'
op|':'
newline|'\n'
indent|'                        '
name|'fragment_payload'
op|'='
op|'['
name|'fragment'
name|'for'
name|'fragment'
name|'in'
name|'pile'
op|']'
newline|'\n'
dedent|''
dedent|''
name|'except'
op|'('
name|'Exception'
op|','
name|'Timeout'
op|')'
op|':'
newline|'\n'
indent|'                    '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Error trying to rebuild %(path)s "'
nl|'\n'
string|'"policy#%(policy)d frag#%(frag_index)s"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'path'"
op|':'
name|'path'
op|','
nl|'\n'
string|"'policy'"
op|':'
name|'policy'
op|','
nl|'\n'
string|"'frag_index'"
op|':'
name|'frag_index'
op|','
nl|'\n'
op|'}'
op|')'
newline|'\n'
name|'break'
newline|'\n'
dedent|''
name|'if'
name|'not'
name|'all'
op|'('
name|'fragment_payload'
op|')'
op|':'
newline|'\n'
indent|'                    '
name|'break'
newline|'\n'
dedent|''
name|'rebuilt_fragment'
op|'='
name|'self'
op|'.'
name|'_reconstruct'
op|'('
nl|'\n'
name|'policy'
op|','
name|'fragment_payload'
op|','
name|'frag_index'
op|')'
newline|'\n'
name|'yield'
name|'rebuilt_fragment'
newline|'\n'
nl|'\n'
dedent|''
dedent|''
name|'return'
name|'fragment_payload_iter'
op|'('
op|')'
newline|'\n'
nl|'\n'
DECL|member|stats_line
dedent|''
name|'def'
name|'stats_line'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Logs various stats for the currently running reconstruction pass.\n        """'
newline|'\n'
name|'if'
op|'('
name|'self'
op|'.'
name|'device_count'
name|'and'
name|'self'
op|'.'
name|'part_count'
name|'and'
nl|'\n'
name|'self'
op|'.'
name|'reconstruction_device_count'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'elapsed'
op|'='
op|'('
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'self'
op|'.'
name|'start'
op|')'
name|'or'
number|'0.000001'
newline|'\n'
name|'rate'
op|'='
name|'self'
op|'.'
name|'reconstruction_part_count'
op|'/'
name|'elapsed'
newline|'\n'
name|'total_part_count'
op|'='
op|'('
name|'self'
op|'.'
name|'part_count'
op|'*'
nl|'\n'
name|'self'
op|'.'
name|'device_count'
op|'/'
nl|'\n'
name|'self'
op|'.'
name|'reconstruction_device_count'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"%(reconstructed)d/%(total)d (%(percentage).2f%%)"'
nl|'\n'
string|'" partitions of %(device)d/%(dtotal)d "'
nl|'\n'
string|'"(%(dpercentage).2f%%) devices"'
nl|'\n'
string|'" reconstructed in %(time).2fs "'
nl|'\n'
string|'"(%(rate).2f/sec, %(remaining)s remaining)"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'reconstructed'"
op|':'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|','
nl|'\n'
string|"'total'"
op|':'
name|'self'
op|'.'
name|'part_count'
op|','
nl|'\n'
string|"'percentage'"
op|':'
nl|'\n'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|'*'
number|'100.0'
op|'/'
name|'self'
op|'.'
name|'part_count'
op|','
nl|'\n'
string|"'device'"
op|':'
name|'self'
op|'.'
name|'reconstruction_device_count'
op|','
nl|'\n'
string|"'dtotal'"
op|':'
name|'self'
op|'.'
name|'device_count'
op|','
nl|'\n'
string|"'dpercentage'"
op|':'
nl|'\n'
name|'self'
op|'.'
name|'reconstruction_device_count'
op|'*'
number|'100.0'
op|'/'
name|'self'
op|'.'
name|'device_count'
op|','
nl|'\n'
string|"'time'"
op|':'
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'self'
op|'.'
name|'start'
op|','
string|"'rate'"
op|':'
name|'rate'
op|','
nl|'\n'
string|"'remaining'"
op|':'
string|"'%d%s'"
op|'%'
nl|'\n'
name|'compute_eta'
op|'('
name|'self'
op|'.'
name|'start'
op|','
nl|'\n'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|','
nl|'\n'
name|'total_part_count'
op|')'
op|'}'
op|')'
newline|'\n'
nl|'\n'
name|'if'
name|'self'
op|'.'
name|'suffix_count'
name|'and'
name|'self'
op|'.'
name|'partition_times'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"%(checked)d suffixes checked - "'
nl|'\n'
string|'"%(hashed).2f%% hashed, %(synced).2f%% synced"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'checked'"
op|':'
name|'self'
op|'.'
name|'suffix_count'
op|','
nl|'\n'
string|"'hashed'"
op|':'
op|'('
name|'self'
op|'.'
name|'suffix_hash'
op|'*'
number|'100.0'
op|')'
op|'/'
name|'self'
op|'.'
name|'suffix_count'
op|','
nl|'\n'
string|"'synced'"
op|':'
op|'('
name|'self'
op|'.'
name|'suffix_sync'
op|'*'
number|'100.0'
op|')'
op|'/'
name|'self'
op|'.'
name|'suffix_count'
op|'}'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'partition_times'
op|'.'
name|'sort'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Partition times: max %(max).4fs, "'
nl|'\n'
string|'"min %(min).4fs, med %(med).4fs"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'max'"
op|':'
name|'self'
op|'.'
name|'partition_times'
op|'['
op|'-'
number|'1'
op|']'
op|','
nl|'\n'
string|"'min'"
op|':'
name|'self'
op|'.'
name|'partition_times'
op|'['
number|'0'
op|']'
op|','
nl|'\n'
string|"'med'"
op|':'
name|'self'
op|'.'
name|'partition_times'
op|'['
nl|'\n'
name|'len'
op|'('
name|'self'
op|'.'
name|'partition_times'
op|')'
op|'//'
number|'2'
op|']'
op|'}'
op|')'
newline|'\n'
dedent|''
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Nothing reconstructed for %s seconds."'
op|')'
op|','
nl|'\n'
op|'('
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'self'
op|'.'
name|'start'
op|')'
op|')'
newline|'\n'
nl|'\n'
DECL|member|kill_coros
dedent|''
dedent|''
name|'def'
name|'kill_coros'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""Utility function that kills all coroutines currently running."""'
newline|'\n'
name|'for'
name|'coro'
name|'in'
name|'list'
op|'('
name|'self'
op|'.'
name|'run_pool'
op|'.'
name|'coroutines_running'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'try'
op|':'
newline|'\n'
indent|'                '
name|'coro'
op|'.'
name|'kill'
op|'('
name|'GreenletExit'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'GreenletExit'
op|':'
newline|'\n'
indent|'                '
name|'pass'
newline|'\n'
nl|'\n'
DECL|member|heartbeat
dedent|''
dedent|''
dedent|''
name|'def'
name|'heartbeat'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Loop that runs in the background during reconstruction.  It\n        periodically logs progress.\n        """'
newline|'\n'
name|'while'
name|'True'
op|':'
newline|'\n'
indent|'            '
name|'sleep'
op|'('
name|'self'
op|'.'
name|'stats_interval'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'stats_line'
op|'('
op|')'
newline|'\n'
nl|'\n'
DECL|member|detect_lockups
dedent|''
dedent|''
name|'def'
name|'detect_lockups'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        In testing, the pool.waitall() call very occasionally failed to return.\n        This is an attempt to make sure the reconstructor finishes its\n        reconstruction pass in some eventuality.\n        """'
newline|'\n'
name|'while'
name|'True'
op|':'
newline|'\n'
indent|'            '
name|'sleep'
op|'('
name|'self'
op|'.'
name|'lockup_timeout'
op|')'
newline|'\n'
name|'if'
name|'self'
op|'.'
name|'reconstruction_count'
op|'=='
name|'self'
op|'.'
name|'last_reconstruction_count'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'error'
op|'('
name|'_'
op|'('
string|'"Lockup detected.. killing live coros."'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'kill_coros'
op|'('
op|')'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'last_reconstruction_count'
op|'='
name|'self'
op|'.'
name|'reconstruction_count'
newline|'\n'
nl|'\n'
DECL|member|_get_hashes
dedent|''
dedent|''
name|'def'
name|'_get_hashes'
op|'('
name|'self'
op|','
name|'policy'
op|','
name|'path'
op|','
name|'recalculate'
op|'='
name|'None'
op|','
name|'do_listdir'
op|'='
name|'False'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'df_mgr'
op|'='
name|'self'
op|'.'
name|'_df_router'
op|'['
name|'policy'
op|']'
newline|'\n'
name|'hashed'
op|','
name|'suffix_hashes'
op|'='
name|'tpool_reraise'
op|'('
nl|'\n'
name|'df_mgr'
op|'.'
name|'_get_hashes'
op|','
name|'path'
op|','
name|'recalculate'
op|'='
name|'recalculate'
op|','
nl|'\n'
name|'do_listdir'
op|'='
name|'do_listdir'
op|','
name|'reclaim_age'
op|'='
name|'self'
op|'.'
name|'reclaim_age'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'update_stats'
op|'('
string|"'suffix.hashes'"
op|','
name|'hashed'
op|')'
newline|'\n'
name|'return'
name|'suffix_hashes'
newline|'\n'
nl|'\n'
DECL|member|get_suffix_delta
dedent|''
name|'def'
name|'get_suffix_delta'
op|'('
name|'self'
op|','
name|'local_suff'
op|','
name|'local_index'
op|','
nl|'\n'
name|'remote_suff'
op|','
name|'remote_index'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Compare the local suffix hashes with the remote suffix hashes\n        for the given local and remote fragment indexes.  Return those\n        suffixes which should be synced.\n\n        :param local_suff: the local suffix hashes (from _get_hashes)\n        :param local_index: the local fragment index for the job\n        :param remote_suff: the remote suffix hashes (from remote\n                            REPLICATE request)\n        :param remote_index: the remote fragment index for the job\n\n        :returns: a list of strings, the suffix dirs to sync\n        """'
newline|'\n'
name|'suffixes'
op|'='
op|'['
op|']'
newline|'\n'
name|'for'
name|'suffix'
op|','
name|'sub_dict_local'
name|'in'
name|'local_suff'
op|'.'
name|'items'
op|'('
op|')'
op|':'
newline|'\n'
indent|'            '
name|'sub_dict_remote'
op|'='
name|'remote_suff'
op|'.'
name|'get'
op|'('
name|'suffix'
op|','
op|'{'
op|'}'
op|')'
newline|'\n'
name|'if'
op|'('
name|'sub_dict_local'
op|'.'
name|'get'
op|'('
name|'None'
op|')'
op|'!='
name|'sub_dict_remote'
op|'.'
name|'get'
op|'('
name|'None'
op|')'
name|'or'
nl|'\n'
name|'sub_dict_local'
op|'.'
name|'get'
op|'('
name|'local_index'
op|')'
op|'!='
nl|'\n'
name|'sub_dict_remote'
op|'.'
name|'get'
op|'('
name|'remote_index'
op|')'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'suffixes'
op|'.'
name|'append'
op|'('
name|'suffix'
op|')'
newline|'\n'
dedent|''
dedent|''
name|'return'
name|'suffixes'
newline|'\n'
nl|'\n'
DECL|member|rehash_remote
dedent|''
name|'def'
name|'rehash_remote'
op|'('
name|'self'
op|','
name|'node'
op|','
name|'job'
op|','
name|'suffixes'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'try'
op|':'
newline|'\n'
indent|'            '
name|'with'
name|'Timeout'
op|'('
name|'self'
op|'.'
name|'http_timeout'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'conn'
op|'='
name|'http_connect'
op|'('
nl|'\n'
name|'node'
op|'['
string|"'replication_ip'"
op|']'
op|','
name|'node'
op|'['
string|"'replication_port'"
op|']'
op|','
nl|'\n'
name|'node'
op|'['
string|"'device'"
op|']'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"'REPLICATE'"
op|','
nl|'\n'
string|"'/'"
op|'+'
string|"'-'"
op|'.'
name|'join'
op|'('
name|'sorted'
op|'('
name|'suffixes'
op|')'
op|')'
op|','
nl|'\n'
name|'headers'
op|'='
name|'self'
op|'.'
name|'headers'
op|')'
newline|'\n'
name|'conn'
op|'.'
name|'getresponse'
op|'('
op|')'
op|'.'
name|'read'
op|'('
op|')'
newline|'\n'
dedent|''
dedent|''
name|'except'
op|'('
name|'Exception'
op|','
name|'Timeout'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Trying to sync suffixes with %s"'
op|')'
op|'%'
name|'self'
op|'.'
name|'_full_path'
op|'('
nl|'\n'
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"''"
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
op|')'
newline|'\n'
nl|'\n'
DECL|member|_get_suffixes_to_sync
dedent|''
dedent|''
name|'def'
name|'_get_suffixes_to_sync'
op|'('
name|'self'
op|','
name|'job'
op|','
name|'node'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        For SYNC jobs we need to make a remote REPLICATE request to get\n        the remote node\'s current suffix\'s hashes and then compare to our\n        local suffix\'s hashes to decide which suffixes (if any) are out\n        of sync.\n\n        :param: the job dict, with the keys defined in ``_get_part_jobs``\n        :param node: the remote node dict\n        :returns: a (possibly empty) list of strings, the suffixes to be\n                  synced with the remote node.\n        """'
newline|'\n'
comment|'# get hashes from the remote node'
nl|'\n'
name|'remote_suffixes'
op|'='
name|'None'
newline|'\n'
name|'try'
op|':'
newline|'\n'
indent|'            '
name|'with'
name|'Timeout'
op|'('
name|'self'
op|'.'
name|'http_timeout'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'resp'
op|'='
name|'http_connect'
op|'('
nl|'\n'
name|'node'
op|'['
string|"'replication_ip'"
op|']'
op|','
name|'node'
op|'['
string|"'replication_port'"
op|']'
op|','
nl|'\n'
name|'node'
op|'['
string|"'device'"
op|']'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"'REPLICATE'"
op|','
nl|'\n'
string|"''"
op|','
name|'headers'
op|'='
name|'self'
op|'.'
name|'headers'
op|')'
op|'.'
name|'getresponse'
op|'('
op|')'
newline|'\n'
dedent|''
name|'if'
name|'resp'
op|'.'
name|'status'
op|'=='
name|'HTTP_INSUFFICIENT_STORAGE'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'error'
op|'('
nl|'\n'
name|'_'
op|'('
string|"'%s responded as unmounted'"
op|')'
op|','
nl|'\n'
name|'self'
op|'.'
name|'_full_path'
op|'('
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"''"
op|','
nl|'\n'
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
op|')'
newline|'\n'
dedent|''
name|'elif'
name|'resp'
op|'.'
name|'status'
op|'!='
name|'HTTP_OK'
op|':'
newline|'\n'
indent|'                '
name|'full_path'
op|'='
name|'self'
op|'.'
name|'_full_path'
op|'('
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"''"
op|','
nl|'\n'
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'error'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Invalid response %(resp)s from %(full_path)s"'
op|')'
op|','
nl|'\n'
op|'{'
string|"'resp'"
op|':'
name|'resp'
op|'.'
name|'status'
op|','
string|"'full_path'"
op|':'
name|'full_path'
op|'}'
op|')'
newline|'\n'
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'                '
name|'remote_suffixes'
op|'='
name|'pickle'
op|'.'
name|'loads'
op|'('
name|'resp'
op|'.'
name|'read'
op|'('
op|')'
op|')'
newline|'\n'
dedent|''
dedent|''
name|'except'
op|'('
name|'Exception'
op|','
name|'Timeout'
op|')'
op|':'
newline|'\n'
comment|'# all exceptions are logged here so that our caller can'
nl|'\n'
comment|'# safely catch our exception and continue to the next node'
nl|'\n'
comment|'# without logging'
nl|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
string|"'Unable to get remote suffix hashes '"
nl|'\n'
string|"'from %r'"
op|'%'
name|'self'
op|'.'
name|'_full_path'
op|'('
nl|'\n'
name|'node'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
string|"''"
op|','
nl|'\n'
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
op|')'
newline|'\n'
nl|'\n'
dedent|''
name|'if'
name|'remote_suffixes'
name|'is'
name|'None'
op|':'
newline|'\n'
indent|'            '
name|'raise'
name|'SuffixSyncError'
op|'('
string|"'Unable to get remote suffix hashes'"
op|')'
newline|'\n'
nl|'\n'
dedent|''
name|'suffixes'
op|'='
name|'self'
op|'.'
name|'get_suffix_delta'
op|'('
name|'job'
op|'['
string|"'hashes'"
op|']'
op|','
nl|'\n'
name|'job'
op|'['
string|"'frag_index'"
op|']'
op|','
nl|'\n'
name|'remote_suffixes'
op|','
nl|'\n'
name|'node'
op|'['
string|"'index'"
op|']'
op|')'
newline|'\n'
comment|"# now recalculate local hashes for suffixes that don't"
nl|'\n'
comment|"# match so we're comparing the latest"
nl|'\n'
name|'local_suff'
op|'='
name|'self'
op|'.'
name|'_get_hashes'
op|'('
name|'job'
op|'['
string|"'policy'"
op|']'
op|','
name|'job'
op|'['
string|"'path'"
op|']'
op|','
nl|'\n'
name|'recalculate'
op|'='
name|'suffixes'
op|')'
newline|'\n'
nl|'\n'
name|'suffixes'
op|'='
name|'self'
op|'.'
name|'get_suffix_delta'
op|'('
name|'local_suff'
op|','
nl|'\n'
name|'job'
op|'['
string|"'frag_index'"
op|']'
op|','
nl|'\n'
name|'remote_suffixes'
op|','
nl|'\n'
name|'node'
op|'['
string|"'index'"
op|']'
op|')'
newline|'\n'
nl|'\n'
name|'self'
op|'.'
name|'suffix_count'
op|'+='
name|'len'
op|'('
name|'suffixes'
op|')'
newline|'\n'
name|'return'
name|'suffixes'
newline|'\n'
nl|'\n'
DECL|member|delete_reverted_objs
dedent|''
name|'def'
name|'delete_reverted_objs'
op|'('
name|'self'
op|','
name|'job'
op|','
name|'objects'
op|','
name|'frag_index'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        For EC we can potentially revert only some of a partition\n        so we\'ll delete reverted objects here. Note that we delete\n        the fragment index of the file we sent to the remote node.\n\n        :param job: the job being processed\n        :param objects: a dict of objects to be deleted, each entry maps\n                        hash=>timestamp\n        :param frag_index: (int) the fragment index of data files to be deleted\n        """'
newline|'\n'
name|'df_mgr'
op|'='
name|'self'
op|'.'
name|'_df_router'
op|'['
name|'job'
op|'['
string|"'policy'"
op|']'
op|']'
newline|'\n'
name|'for'
name|'object_hash'
op|','
name|'timestamps'
name|'in'
name|'objects'
op|'.'
name|'items'
op|'('
op|')'
op|':'
newline|'\n'
indent|'            '
name|'try'
op|':'
newline|'\n'
indent|'                '
name|'df'
op|'='
name|'df_mgr'
op|'.'
name|'get_diskfile_from_hash'
op|'('
nl|'\n'
name|'job'
op|'['
string|"'local_dev'"
op|']'
op|'['
string|"'device'"
op|']'
op|','
name|'job'
op|'['
string|"'partition'"
op|']'
op|','
nl|'\n'
name|'object_hash'
op|','
name|'job'
op|'['
string|"'policy'"
op|']'
op|','
nl|'\n'
name|'frag_index'
op|'='
name|'frag_index'
op|')'
newline|'\n'
name|'df'
op|'.'
name|'purge'
op|'('
name|'timestamps'
op|'['
string|"'ts_data'"
op|']'
op|','
name|'frag_index'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'DiskFileError'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
string|"'Unable to purge DiskFile (%r %r %r)'"
op|','
nl|'\n'
name|'object_hash'
op|','
name|'timestamps'
op|'['
string|"'ts_data'"
op|']'
op|','
name|'frag_index'
op|')'
newline|'\n'
name|'continue'
newline|'\n'
nl|'\n'
DECL|member|process_job
dedent|''
dedent|''
dedent|''
name|'def'
name|'process_job'
op|'('
name|'self'
op|','
name|'job'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Sync the local partition with the remote node(s) according to\n        the parameters of the job.  For primary nodes, the SYNC job type\n        will define both left and right hand sync_to nodes to ssync with\n        as defined by this primary nodes index in the node list based on\n        the fragment index found in the partition.  For non-primary\n        nodes (either handoff revert, or rebalance) the REVERT job will\n        define a single node in sync_to which is the proper/new home for\n        the fragment index.\n\n        N.B. ring rebalancing can be time consuming and handoff nodes\'\n        fragment indexes do not have a stable order, it\'s possible to\n        have more than one REVERT job for a partition, and in some rare\n        failure conditions there may even also be a SYNC job for the\n        same partition - but each one will be processed separately\n        because each job will define a separate list of node(s) to\n        \'sync_to\'.\n\n        :param: the job dict, with the keys defined in ``_get_job_info``\n        """'
newline|'\n'
name|'self'
op|'.'
name|'headers'
op|'['
string|"'X-Backend-Storage-Policy-Index'"
op|']'
op|'='
name|'int'
op|'('
name|'job'
op|'['
string|"'policy'"
op|']'
op|')'
newline|'\n'
name|'begin'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
newline|'\n'
name|'if'
name|'job'
op|'['
string|"'job_type'"
op|']'
op|'=='
name|'REVERT'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'_revert'
op|'('
name|'job'
op|','
name|'begin'
op|')'
newline|'\n'
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'_sync'
op|'('
name|'job'
op|','
name|'begin'
op|')'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'partition_times'
op|'.'
name|'append'
op|'('
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'begin'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_count'
op|'+='
number|'1'
newline|'\n'
nl|'\n'
DECL|member|_sync
dedent|''
name|'def'
name|'_sync'
op|'('
name|'self'
op|','
name|'job'
op|','
name|'begin'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Process a SYNC job.\n        """'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'increment'
op|'('
nl|'\n'
string|"'partition.update.count.%s'"
op|'%'
op|'('
name|'job'
op|'['
string|"'local_dev'"
op|']'
op|'['
string|"'device'"
op|']'
op|','
op|')'
op|')'
newline|'\n'
comment|"# after our left and right partners, if there's some sort of"
nl|'\n'
comment|"# failure we'll continue onto the remaining primary nodes and"
nl|'\n'
comment|"# make sure they're in sync - or potentially rebuild missing"
nl|'\n'
comment|'# fragments we find'
nl|'\n'
name|'dest_nodes'
op|'='
name|'itertools'
op|'.'
name|'chain'
op|'('
nl|'\n'
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|','
nl|'\n'
comment|'# I think we could order these based on our index to better'
nl|'\n'
comment|'# protect against a broken chain'
nl|'\n'
op|'['
nl|'\n'
name|'n'
name|'for'
name|'n'
name|'in'
nl|'\n'
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'object_ring'
op|'.'
name|'get_part_nodes'
op|'('
name|'job'
op|'['
string|"'partition'"
op|']'
op|')'
nl|'\n'
name|'if'
name|'n'
op|'['
string|"'id'"
op|']'
op|'!='
name|'job'
op|'['
string|"'local_dev'"
op|']'
op|'['
string|"'id'"
op|']'
name|'and'
nl|'\n'
name|'n'
op|'['
string|"'id'"
op|']'
name|'not'
name|'in'
op|'('
name|'m'
op|'['
string|"'id'"
op|']'
name|'for'
name|'m'
name|'in'
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|')'
nl|'\n'
op|']'
op|','
nl|'\n'
op|')'
newline|'\n'
name|'syncd_with'
op|'='
number|'0'
newline|'\n'
name|'for'
name|'node'
name|'in'
name|'dest_nodes'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'syncd_with'
op|'>='
name|'len'
op|'('
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|')'
op|':'
newline|'\n'
comment|'# success!'
nl|'\n'
indent|'                '
name|'break'
newline|'\n'
nl|'\n'
dedent|''
name|'try'
op|':'
newline|'\n'
indent|'                '
name|'suffixes'
op|'='
name|'self'
op|'.'
name|'_get_suffixes_to_sync'
op|'('
name|'job'
op|','
name|'node'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'SuffixSyncError'
op|':'
newline|'\n'
indent|'                '
name|'continue'
newline|'\n'
nl|'\n'
dedent|''
name|'if'
name|'not'
name|'suffixes'
op|':'
newline|'\n'
indent|'                '
name|'syncd_with'
op|'+='
number|'1'
newline|'\n'
name|'continue'
newline|'\n'
nl|'\n'
comment|'# ssync any out-of-sync suffixes with the remote node'
nl|'\n'
dedent|''
name|'success'
op|','
name|'_'
op|'='
name|'ssync_sender'
op|'('
nl|'\n'
name|'self'
op|','
name|'node'
op|','
name|'job'
op|','
name|'suffixes'
op|')'
op|'('
op|')'
newline|'\n'
comment|"# let remote end know to rehash it's suffixes"
nl|'\n'
name|'self'
op|'.'
name|'rehash_remote'
op|'('
name|'node'
op|','
name|'job'
op|','
name|'suffixes'
op|')'
newline|'\n'
comment|'# update stats for this attempt'
nl|'\n'
name|'self'
op|'.'
name|'suffix_sync'
op|'+='
name|'len'
op|'('
name|'suffixes'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'update_stats'
op|'('
string|"'suffix.syncs'"
op|','
name|'len'
op|'('
name|'suffixes'
op|')'
op|')'
newline|'\n'
name|'if'
name|'success'
op|':'
newline|'\n'
indent|'                '
name|'syncd_with'
op|'+='
number|'1'
newline|'\n'
dedent|''
dedent|''
name|'self'
op|'.'
name|'logger'
op|'.'
name|'timing_since'
op|'('
string|"'partition.update.timing'"
op|','
name|'begin'
op|')'
newline|'\n'
nl|'\n'
DECL|member|_revert
dedent|''
name|'def'
name|'_revert'
op|'('
name|'self'
op|','
name|'job'
op|','
name|'begin'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Process a REVERT job.\n        """'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'increment'
op|'('
nl|'\n'
string|"'partition.delete.count.%s'"
op|'%'
op|'('
name|'job'
op|'['
string|"'local_dev'"
op|']'
op|'['
string|"'device'"
op|']'
op|','
op|')'
op|')'
newline|'\n'
comment|"# we'd desperately like to push this partition back to it's"
nl|'\n'
comment|'# primary location, but if that node is down, the next best thing'
nl|'\n'
comment|'# is one of the handoff locations - which *might* be us already!'
nl|'\n'
name|'dest_nodes'
op|'='
name|'itertools'
op|'.'
name|'chain'
op|'('
nl|'\n'
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|','
nl|'\n'
name|'job'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'object_ring'
op|'.'
name|'get_more_nodes'
op|'('
name|'job'
op|'['
string|"'partition'"
op|']'
op|')'
op|','
nl|'\n'
op|')'
newline|'\n'
name|'syncd_with'
op|'='
number|'0'
newline|'\n'
name|'reverted_objs'
op|'='
op|'{'
op|'}'
newline|'\n'
name|'for'
name|'node'
name|'in'
name|'dest_nodes'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'syncd_with'
op|'>='
name|'len'
op|'('
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'break'
newline|'\n'
dedent|''
name|'if'
name|'node'
op|'['
string|"'id'"
op|']'
op|'=='
name|'job'
op|'['
string|"'local_dev'"
op|']'
op|'['
string|"'id'"
op|']'
op|':'
newline|'\n'
comment|'# this is as good a place as any for this data for now'
nl|'\n'
indent|'                '
name|'break'
newline|'\n'
dedent|''
name|'success'
op|','
name|'in_sync_objs'
op|'='
name|'ssync_sender'
op|'('
nl|'\n'
name|'self'
op|','
name|'node'
op|','
name|'job'
op|','
name|'job'
op|'['
string|"'suffixes'"
op|']'
op|')'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'rehash_remote'
op|'('
name|'node'
op|','
name|'job'
op|','
name|'job'
op|'['
string|"'suffixes'"
op|']'
op|')'
newline|'\n'
name|'if'
name|'success'
op|':'
newline|'\n'
indent|'                '
name|'syncd_with'
op|'+='
number|'1'
newline|'\n'
name|'reverted_objs'
op|'.'
name|'update'
op|'('
name|'in_sync_objs'
op|')'
newline|'\n'
dedent|''
dedent|''
name|'if'
name|'syncd_with'
op|'>='
name|'len'
op|'('
name|'job'
op|'['
string|"'sync_to'"
op|']'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'delete_reverted_objs'
op|'('
nl|'\n'
name|'job'
op|','
name|'reverted_objs'
op|','
name|'job'
op|'['
string|"'frag_index'"
op|']'
op|')'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'logger'
op|'.'
name|'timing_since'
op|'('
string|"'partition.delete.timing'"
op|','
name|'begin'
op|')'
newline|'\n'
nl|'\n'
DECL|member|_get_part_jobs
dedent|''
name|'def'
name|'_get_part_jobs'
op|'('
name|'self'
op|','
name|'local_dev'
op|','
name|'part_path'
op|','
name|'partition'
op|','
name|'policy'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Helper function to build jobs for a partition, this method will\n        read the suffix hashes and create job dictionaries to describe\n        the needed work.  There will be one job for each fragment index\n        discovered in the partition.\n\n        For a fragment index which corresponds to this node\'s ring\n        index, a job with job_type SYNC will be created to ensure that\n        the left and right hand primary ring nodes for the part have the\n        corresponding left and right hand fragment archives.\n\n        A fragment index (or entire partition) for which this node is\n        not the primary corresponding node, will create job(s) with\n        job_type REVERT to ensure that fragment archives are pushed to\n        the correct node and removed from this one.\n\n        A partition may result in multiple jobs.  Potentially many\n        REVERT jobs, and zero or one SYNC job.\n\n        :param local_dev:  the local device\n        :param part_path: full path to partition\n        :param partition: partition number\n        :param policy: the policy\n\n        :returns: a list of dicts of job info\n        """'
newline|'\n'
comment|"# find all the fi's in the part, and which suffixes have them"
nl|'\n'
name|'hashes'
op|'='
name|'self'
op|'.'
name|'_get_hashes'
op|'('
name|'policy'
op|','
name|'part_path'
op|','
name|'do_listdir'
op|'='
name|'True'
op|')'
newline|'\n'
name|'non_data_fragment_suffixes'
op|'='
op|'['
op|']'
newline|'\n'
name|'data_fi_to_suffixes'
op|'='
name|'defaultdict'
op|'('
name|'list'
op|')'
newline|'\n'
name|'for'
name|'suffix'
op|','
name|'fi_hash'
name|'in'
name|'hashes'
op|'.'
name|'items'
op|'('
op|')'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'not'
name|'fi_hash'
op|':'
newline|'\n'
comment|'# this is for sanity and clarity, normally an empty'
nl|'\n'
comment|"# suffix would get del'd from the hashes dict, but an"
nl|'\n'
comment|'# OSError trying to re-hash the suffix could leave the'
nl|'\n'
comment|"# value empty - it will log the exception; but there's"
nl|'\n'
comment|'# no way to properly address this suffix at this time.'
nl|'\n'
indent|'                '
name|'continue'
newline|'\n'
dedent|''
name|'data_frag_indexes'
op|'='
op|'['
name|'f'
name|'for'
name|'f'
name|'in'
name|'fi_hash'
name|'if'
name|'f'
name|'is'
name|'not'
name|'None'
op|']'
newline|'\n'
name|'if'
name|'not'
name|'data_frag_indexes'
op|':'
newline|'\n'
indent|'                '
name|'non_data_fragment_suffixes'
op|'.'
name|'append'
op|'('
name|'suffix'
op|')'
newline|'\n'
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'                '
name|'for'
name|'fi'
name|'in'
name|'data_frag_indexes'
op|':'
newline|'\n'
indent|'                    '
name|'data_fi_to_suffixes'
op|'['
name|'fi'
op|']'
op|'.'
name|'append'
op|'('
name|'suffix'
op|')'
newline|'\n'
nl|'\n'
comment|'# helper to ensure consistent structure of jobs'
nl|'\n'
DECL|function|build_job
dedent|''
dedent|''
dedent|''
name|'def'
name|'build_job'
op|'('
name|'job_type'
op|','
name|'frag_index'
op|','
name|'suffixes'
op|','
name|'sync_to'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'return'
op|'{'
nl|'\n'
string|"'job_type'"
op|':'
name|'job_type'
op|','
nl|'\n'
string|"'frag_index'"
op|':'
name|'frag_index'
op|','
nl|'\n'
string|"'suffixes'"
op|':'
name|'suffixes'
op|','
nl|'\n'
string|"'sync_to'"
op|':'
name|'sync_to'
op|','
nl|'\n'
string|"'partition'"
op|':'
name|'partition'
op|','
nl|'\n'
string|"'path'"
op|':'
name|'part_path'
op|','
nl|'\n'
string|"'hashes'"
op|':'
name|'hashes'
op|','
nl|'\n'
string|"'policy'"
op|':'
name|'policy'
op|','
nl|'\n'
string|"'local_dev'"
op|':'
name|'local_dev'
op|','
nl|'\n'
comment|'# ssync likes to have it handy'
nl|'\n'
string|"'device'"
op|':'
name|'local_dev'
op|'['
string|"'device'"
op|']'
op|','
nl|'\n'
op|'}'
newline|'\n'
nl|'\n'
comment|'# aggregate jobs for all the fragment index in this part'
nl|'\n'
dedent|''
name|'jobs'
op|'='
op|'['
op|']'
newline|'\n'
nl|'\n'
comment|'# check the primary nodes - to see if the part belongs here'
nl|'\n'
name|'part_nodes'
op|'='
name|'policy'
op|'.'
name|'object_ring'
op|'.'
name|'get_part_nodes'
op|'('
name|'partition'
op|')'
newline|'\n'
name|'for'
name|'node'
name|'in'
name|'part_nodes'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'node'
op|'['
string|"'id'"
op|']'
op|'=='
name|'local_dev'
op|'['
string|"'id'"
op|']'
op|':'
newline|'\n'
comment|"# this partition belongs here, we'll need a sync job"
nl|'\n'
indent|'                '
name|'frag_index'
op|'='
name|'node'
op|'['
string|"'index'"
op|']'
newline|'\n'
name|'try'
op|':'
newline|'\n'
indent|'                    '
name|'suffixes'
op|'='
name|'data_fi_to_suffixes'
op|'.'
name|'pop'
op|'('
name|'frag_index'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'KeyError'
op|':'
newline|'\n'
indent|'                    '
name|'suffixes'
op|'='
op|'['
op|']'
newline|'\n'
dedent|''
name|'sync_job'
op|'='
name|'build_job'
op|'('
nl|'\n'
name|'job_type'
op|'='
name|'SYNC'
op|','
nl|'\n'
name|'frag_index'
op|'='
name|'frag_index'
op|','
nl|'\n'
name|'suffixes'
op|'='
name|'suffixes'
op|','
nl|'\n'
name|'sync_to'
op|'='
name|'_get_partners'
op|'('
name|'frag_index'
op|','
name|'part_nodes'
op|')'
op|','
nl|'\n'
op|')'
newline|'\n'
comment|'# ssync callback to rebuild missing fragment_archives'
nl|'\n'
name|'sync_job'
op|'['
string|"'sync_diskfile_builder'"
op|']'
op|'='
name|'self'
op|'.'
name|'reconstruct_fa'
newline|'\n'
name|'jobs'
op|'.'
name|'append'
op|'('
name|'sync_job'
op|')'
newline|'\n'
name|'break'
newline|'\n'
nl|'\n'
comment|'# assign remaining data fragment suffixes to revert jobs'
nl|'\n'
dedent|''
dedent|''
name|'ordered_fis'
op|'='
name|'sorted'
op|'('
op|'('
name|'len'
op|'('
name|'suffixes'
op|')'
op|','
name|'fi'
op|')'
name|'for'
name|'fi'
op|','
name|'suffixes'
nl|'\n'
name|'in'
name|'data_fi_to_suffixes'
op|'.'
name|'items'
op|'('
op|')'
op|')'
newline|'\n'
name|'for'
name|'count'
op|','
name|'fi'
name|'in'
name|'ordered_fis'
op|':'
newline|'\n'
indent|'            '
name|'revert_job'
op|'='
name|'build_job'
op|'('
nl|'\n'
name|'job_type'
op|'='
name|'REVERT'
op|','
nl|'\n'
name|'frag_index'
op|'='
name|'fi'
op|','
nl|'\n'
name|'suffixes'
op|'='
name|'data_fi_to_suffixes'
op|'['
name|'fi'
op|']'
op|','
nl|'\n'
name|'sync_to'
op|'='
op|'['
name|'part_nodes'
op|'['
name|'fi'
op|']'
op|']'
op|','
nl|'\n'
op|')'
newline|'\n'
name|'jobs'
op|'.'
name|'append'
op|'('
name|'revert_job'
op|')'
newline|'\n'
nl|'\n'
comment|'# now we need to assign suffixes that have no data fragments'
nl|'\n'
dedent|''
name|'if'
name|'non_data_fragment_suffixes'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'jobs'
op|':'
newline|'\n'
comment|'# the first job will be either the sync_job, or the'
nl|'\n'
comment|'# revert_job for the fragment index that is most common'
nl|'\n'
comment|'# among the suffixes'
nl|'\n'
indent|'                '
name|'jobs'
op|'['
number|'0'
op|']'
op|'['
string|"'suffixes'"
op|']'
op|'.'
name|'extend'
op|'('
name|'non_data_fragment_suffixes'
op|')'
newline|'\n'
dedent|''
name|'else'
op|':'
newline|'\n'
comment|'# this is an unfortunate situation, we need a revert job to'
nl|'\n'
comment|'# push partitions off this node, but none of the suffixes'
nl|'\n'
comment|'# have any data fragments to hint at which node would be a'
nl|'\n'
comment|'# good candidate to receive the tombstones.'
nl|'\n'
indent|'                '
name|'jobs'
op|'.'
name|'append'
op|'('
name|'build_job'
op|'('
nl|'\n'
name|'job_type'
op|'='
name|'REVERT'
op|','
nl|'\n'
name|'frag_index'
op|'='
name|'None'
op|','
nl|'\n'
name|'suffixes'
op|'='
name|'non_data_fragment_suffixes'
op|','
nl|'\n'
comment|'# this is super safe'
nl|'\n'
name|'sync_to'
op|'='
name|'part_nodes'
op|','
nl|'\n'
comment|'# something like this would be probably be better'
nl|'\n'
comment|'# sync_to=random.sample(part_nodes, 3),'
nl|'\n'
op|')'
op|')'
newline|'\n'
comment|'# return a list of jobs for this part'
nl|'\n'
dedent|''
dedent|''
name|'return'
name|'jobs'
newline|'\n'
nl|'\n'
DECL|member|collect_parts
dedent|''
name|'def'
name|'collect_parts'
op|'('
name|'self'
op|','
name|'override_devices'
op|'='
name|'None'
op|','
nl|'\n'
name|'override_partitions'
op|'='
name|'None'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Helper for yielding partitions in the top level reconstructor\n        """'
newline|'\n'
name|'override_devices'
op|'='
name|'override_devices'
name|'or'
op|'['
op|']'
newline|'\n'
name|'override_partitions'
op|'='
name|'override_partitions'
name|'or'
op|'['
op|']'
newline|'\n'
name|'ips'
op|'='
name|'whataremyips'
op|'('
name|'self'
op|'.'
name|'bind_ip'
op|')'
newline|'\n'
name|'for'
name|'policy'
name|'in'
name|'POLICIES'
op|':'
newline|'\n'
indent|'            '
name|'if'
name|'policy'
op|'.'
name|'policy_type'
op|'!='
name|'EC_POLICY'
op|':'
newline|'\n'
indent|'                '
name|'continue'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'_diskfile_mgr'
op|'='
name|'self'
op|'.'
name|'_df_router'
op|'['
name|'policy'
op|']'
newline|'\n'
name|'self'
op|'.'
name|'load_object_ring'
op|'('
name|'policy'
op|')'
newline|'\n'
name|'data_dir'
op|'='
name|'get_data_dir'
op|'('
name|'policy'
op|')'
newline|'\n'
name|'local_devices'
op|'='
name|'list'
op|'('
name|'six'
op|'.'
name|'moves'
op|'.'
name|'filter'
op|'('
nl|'\n'
name|'lambda'
name|'dev'
op|':'
name|'dev'
name|'and'
name|'is_local_device'
op|'('
nl|'\n'
name|'ips'
op|','
name|'self'
op|'.'
name|'port'
op|','
nl|'\n'
name|'dev'
op|'['
string|"'replication_ip'"
op|']'
op|','
name|'dev'
op|'['
string|"'replication_port'"
op|']'
op|')'
op|','
nl|'\n'
name|'policy'
op|'.'
name|'object_ring'
op|'.'
name|'devs'
op|')'
op|')'
newline|'\n'
nl|'\n'
name|'if'
name|'override_devices'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'device_count'
op|'='
name|'len'
op|'('
name|'override_devices'
op|')'
newline|'\n'
dedent|''
name|'else'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'device_count'
op|'='
name|'len'
op|'('
name|'local_devices'
op|')'
newline|'\n'
nl|'\n'
dedent|''
name|'for'
name|'local_dev'
name|'in'
name|'local_devices'
op|':'
newline|'\n'
indent|'                '
name|'if'
name|'override_devices'
name|'and'
op|'('
name|'local_dev'
op|'['
string|"'device'"
op|']'
name|'not'
name|'in'
nl|'\n'
name|'override_devices'
op|')'
op|':'
newline|'\n'
indent|'                    '
name|'continue'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'reconstruction_device_count'
op|'+='
number|'1'
newline|'\n'
name|'dev_path'
op|'='
name|'self'
op|'.'
name|'_df_router'
op|'['
name|'policy'
op|']'
op|'.'
name|'get_dev_path'
op|'('
nl|'\n'
name|'local_dev'
op|'['
string|"'device'"
op|']'
op|')'
newline|'\n'
name|'if'
name|'not'
name|'dev_path'
op|':'
newline|'\n'
indent|'                    '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'warning'
op|'('
name|'_'
op|'('
string|"'%s is not mounted'"
op|')'
op|','
nl|'\n'
name|'local_dev'
op|'['
string|"'device'"
op|']'
op|')'
newline|'\n'
name|'continue'
newline|'\n'
dedent|''
name|'obj_path'
op|'='
name|'join'
op|'('
name|'dev_path'
op|','
name|'data_dir'
op|')'
newline|'\n'
name|'tmp_path'
op|'='
name|'join'
op|'('
name|'dev_path'
op|','
name|'get_tmp_dir'
op|'('
name|'int'
op|'('
name|'policy'
op|')'
op|')'
op|')'
newline|'\n'
name|'unlink_older_than'
op|'('
name|'tmp_path'
op|','
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
nl|'\n'
name|'self'
op|'.'
name|'reclaim_age'
op|')'
newline|'\n'
name|'if'
name|'not'
name|'os'
op|'.'
name|'path'
op|'.'
name|'exists'
op|'('
name|'obj_path'
op|')'
op|':'
newline|'\n'
indent|'                    '
name|'try'
op|':'
newline|'\n'
indent|'                        '
name|'mkdirs'
op|'('
name|'obj_path'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'Exception'
op|':'
newline|'\n'
indent|'                        '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
string|"'Unable to create %s'"
op|'%'
name|'obj_path'
op|')'
newline|'\n'
dedent|''
name|'continue'
newline|'\n'
dedent|''
name|'try'
op|':'
newline|'\n'
indent|'                    '
name|'partitions'
op|'='
name|'os'
op|'.'
name|'listdir'
op|'('
name|'obj_path'
op|')'
newline|'\n'
dedent|''
name|'except'
name|'OSError'
op|':'
newline|'\n'
indent|'                    '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
nl|'\n'
string|"'Unable to list partitions in %r'"
op|'%'
name|'obj_path'
op|')'
newline|'\n'
name|'continue'
newline|'\n'
nl|'\n'
dedent|''
name|'self'
op|'.'
name|'part_count'
op|'+='
name|'len'
op|'('
name|'partitions'
op|')'
newline|'\n'
name|'for'
name|'partition'
name|'in'
name|'partitions'
op|':'
newline|'\n'
indent|'                    '
name|'part_path'
op|'='
name|'join'
op|'('
name|'obj_path'
op|','
name|'partition'
op|')'
newline|'\n'
name|'if'
name|'not'
op|'('
name|'partition'
op|'.'
name|'isdigit'
op|'('
op|')'
name|'and'
nl|'\n'
name|'os'
op|'.'
name|'path'
op|'.'
name|'isdir'
op|'('
name|'part_path'
op|')'
op|')'
op|':'
newline|'\n'
indent|'                        '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'warning'
op|'('
nl|'\n'
string|"'Unexpected entity in data dir: %r'"
op|'%'
name|'part_path'
op|')'
newline|'\n'
name|'remove_file'
op|'('
name|'part_path'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|'+='
number|'1'
newline|'\n'
name|'continue'
newline|'\n'
dedent|''
name|'partition'
op|'='
name|'int'
op|'('
name|'partition'
op|')'
newline|'\n'
name|'if'
name|'override_partitions'
name|'and'
op|'('
name|'partition'
name|'not'
name|'in'
nl|'\n'
name|'override_partitions'
op|')'
op|':'
newline|'\n'
indent|'                        '
name|'continue'
newline|'\n'
dedent|''
name|'part_info'
op|'='
op|'{'
nl|'\n'
string|"'local_dev'"
op|':'
name|'local_dev'
op|','
nl|'\n'
string|"'policy'"
op|':'
name|'policy'
op|','
nl|'\n'
string|"'partition'"
op|':'
name|'partition'
op|','
nl|'\n'
string|"'part_path'"
op|':'
name|'part_path'
op|','
nl|'\n'
op|'}'
newline|'\n'
name|'yield'
name|'part_info'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|'+='
number|'1'
newline|'\n'
nl|'\n'
DECL|member|build_reconstruction_jobs
dedent|''
dedent|''
dedent|''
dedent|''
name|'def'
name|'build_reconstruction_jobs'
op|'('
name|'self'
op|','
name|'part_info'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""\n        Helper function for collect_jobs to build jobs for reconstruction\n        using EC style storage policy\n        """'
newline|'\n'
name|'jobs'
op|'='
name|'self'
op|'.'
name|'_get_part_jobs'
op|'('
op|'**'
name|'part_info'
op|')'
newline|'\n'
name|'random'
op|'.'
name|'shuffle'
op|'('
name|'jobs'
op|')'
newline|'\n'
name|'if'
name|'self'
op|'.'
name|'handoffs_first'
op|':'
newline|'\n'
comment|'# Move the handoff revert jobs to the front of the list'
nl|'\n'
indent|'            '
name|'jobs'
op|'.'
name|'sort'
op|'('
name|'key'
op|'='
name|'lambda'
name|'job'
op|':'
name|'job'
op|'['
string|"'job_type'"
op|']'
op|','
name|'reverse'
op|'='
name|'True'
op|')'
newline|'\n'
dedent|''
name|'self'
op|'.'
name|'job_count'
op|'+='
name|'len'
op|'('
name|'jobs'
op|')'
newline|'\n'
name|'return'
name|'jobs'
newline|'\n'
nl|'\n'
DECL|member|_reset_stats
dedent|''
name|'def'
name|'_reset_stats'
op|'('
name|'self'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'self'
op|'.'
name|'start'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'job_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'part_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'device_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'suffix_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'suffix_sync'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'suffix_hash'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_part_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'reconstruction_device_count'
op|'='
number|'0'
newline|'\n'
name|'self'
op|'.'
name|'last_reconstruction_count'
op|'='
op|'-'
number|'1'
newline|'\n'
nl|'\n'
DECL|member|delete_partition
dedent|''
name|'def'
name|'delete_partition'
op|'('
name|'self'
op|','
name|'path'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
name|'_'
op|'('
string|'"Removing partition: %s"'
op|')'
op|','
name|'path'
op|')'
newline|'\n'
name|'tpool'
op|'.'
name|'execute'
op|'('
name|'shutil'
op|'.'
name|'rmtree'
op|','
name|'path'
op|','
name|'ignore_errors'
op|'='
name|'True'
op|')'
newline|'\n'
nl|'\n'
DECL|member|reconstruct
dedent|''
name|'def'
name|'reconstruct'
op|'('
name|'self'
op|','
op|'**'
name|'kwargs'
op|')'
op|':'
newline|'\n'
indent|'        '
string|'"""Run a reconstruction pass"""'
newline|'\n'
name|'self'
op|'.'
name|'_reset_stats'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'partition_times'
op|'='
op|'['
op|']'
newline|'\n'
nl|'\n'
name|'stats'
op|'='
name|'spawn'
op|'('
name|'self'
op|'.'
name|'heartbeat'
op|')'
newline|'\n'
name|'lockup_detector'
op|'='
name|'spawn'
op|'('
name|'self'
op|'.'
name|'detect_lockups'
op|')'
newline|'\n'
name|'sleep'
op|'('
op|')'
comment|'# Give spawns a cycle'
newline|'\n'
nl|'\n'
name|'try'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'run_pool'
op|'='
name|'GreenPool'
op|'('
name|'size'
op|'='
name|'self'
op|'.'
name|'concurrency'
op|')'
newline|'\n'
name|'for'
name|'part_info'
name|'in'
name|'self'
op|'.'
name|'collect_parts'
op|'('
op|'**'
name|'kwargs'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'if'
name|'not'
name|'self'
op|'.'
name|'check_ring'
op|'('
name|'part_info'
op|'['
string|"'policy'"
op|']'
op|'.'
name|'object_ring'
op|')'
op|':'
newline|'\n'
indent|'                    '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
name|'_'
op|'('
string|'"Ring change detected. Aborting "'
nl|'\n'
string|'"current reconstruction pass."'
op|')'
op|')'
newline|'\n'
name|'return'
newline|'\n'
dedent|''
name|'jobs'
op|'='
name|'self'
op|'.'
name|'build_reconstruction_jobs'
op|'('
name|'part_info'
op|')'
newline|'\n'
name|'if'
name|'not'
name|'jobs'
op|':'
newline|'\n'
comment|'# If this part belongs on this node, _get_part_jobs'
nl|'\n'
comment|"# will *always* build a sync_job - even if there's"
nl|'\n'
comment|'# no suffixes in the partition that needs to sync.'
nl|'\n'
comment|"# If there's any suffixes in the partition then our"
nl|'\n'
comment|'# job list would have *at least* one revert job.'
nl|'\n'
comment|"# Therefore we know this part a) doesn't belong on"
nl|'\n'
comment|"# this node and b) doesn't have any suffixes in it."
nl|'\n'
indent|'                    '
name|'self'
op|'.'
name|'run_pool'
op|'.'
name|'spawn'
op|'('
name|'self'
op|'.'
name|'delete_partition'
op|','
nl|'\n'
name|'part_info'
op|'['
string|"'part_path'"
op|']'
op|')'
newline|'\n'
dedent|''
name|'for'
name|'job'
name|'in'
name|'jobs'
op|':'
newline|'\n'
indent|'                    '
name|'self'
op|'.'
name|'run_pool'
op|'.'
name|'spawn'
op|'('
name|'self'
op|'.'
name|'process_job'
op|','
name|'job'
op|')'
newline|'\n'
dedent|''
dedent|''
name|'with'
name|'Timeout'
op|'('
name|'self'
op|'.'
name|'lockup_timeout'
op|')'
op|':'
newline|'\n'
indent|'                '
name|'self'
op|'.'
name|'run_pool'
op|'.'
name|'waitall'
op|'('
op|')'
newline|'\n'
dedent|''
dedent|''
name|'except'
op|'('
name|'Exception'
op|','
name|'Timeout'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'exception'
op|'('
name|'_'
op|'('
string|'"Exception in top-level"'
nl|'\n'
string|'"reconstruction loop"'
op|')'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'kill_coros'
op|'('
op|')'
newline|'\n'
dedent|''
name|'finally'
op|':'
newline|'\n'
indent|'            '
name|'stats'
op|'.'
name|'kill'
op|'('
op|')'
newline|'\n'
name|'lockup_detector'
op|'.'
name|'kill'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'stats_line'
op|'('
op|')'
newline|'\n'
nl|'\n'
DECL|member|run_once
dedent|''
dedent|''
name|'def'
name|'run_once'
op|'('
name|'self'
op|','
op|'*'
name|'args'
op|','
op|'**'
name|'kwargs'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'start'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
name|'_'
op|'('
string|'"Running object reconstructor in script mode."'
op|')'
op|')'
newline|'\n'
name|'override_devices'
op|'='
name|'list_from_csv'
op|'('
name|'kwargs'
op|'.'
name|'get'
op|'('
string|"'devices'"
op|')'
op|')'
newline|'\n'
name|'override_partitions'
op|'='
op|'['
name|'int'
op|'('
name|'p'
op|')'
name|'for'
name|'p'
name|'in'
nl|'\n'
name|'list_from_csv'
op|'('
name|'kwargs'
op|'.'
name|'get'
op|'('
string|"'partitions'"
op|')'
op|')'
op|']'
newline|'\n'
name|'self'
op|'.'
name|'reconstruct'
op|'('
nl|'\n'
name|'override_devices'
op|'='
name|'override_devices'
op|','
nl|'\n'
name|'override_partitions'
op|'='
name|'override_partitions'
op|')'
newline|'\n'
name|'total'
op|'='
op|'('
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'start'
op|')'
op|'/'
number|'60'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Object reconstruction complete (once). (%.02f minutes)"'
op|')'
op|','
name|'total'
op|')'
newline|'\n'
name|'if'
name|'not'
op|'('
name|'override_partitions'
name|'or'
name|'override_devices'
op|')'
op|':'
newline|'\n'
indent|'            '
name|'dump_recon_cache'
op|'('
op|'{'
string|"'object_reconstruction_time'"
op|':'
name|'total'
op|','
nl|'\n'
string|"'object_reconstruction_last'"
op|':'
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'}'
op|','
nl|'\n'
name|'self'
op|'.'
name|'rcache'
op|','
name|'self'
op|'.'
name|'logger'
op|')'
newline|'\n'
nl|'\n'
DECL|member|run_forever
dedent|''
dedent|''
name|'def'
name|'run_forever'
op|'('
name|'self'
op|','
op|'*'
name|'args'
op|','
op|'**'
name|'kwargs'
op|')'
op|':'
newline|'\n'
indent|'        '
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
name|'_'
op|'('
string|'"Starting object reconstructor in daemon mode."'
op|')'
op|')'
newline|'\n'
comment|'# Run the reconstructor continually'
nl|'\n'
name|'while'
name|'True'
op|':'
newline|'\n'
indent|'            '
name|'start'
op|'='
name|'time'
op|'.'
name|'time'
op|'('
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
name|'_'
op|'('
string|'"Starting object reconstruction pass."'
op|')'
op|')'
newline|'\n'
comment|'# Run the reconstructor'
nl|'\n'
name|'self'
op|'.'
name|'reconstruct'
op|'('
op|')'
newline|'\n'
name|'total'
op|'='
op|'('
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'-'
name|'start'
op|')'
op|'/'
number|'60'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'info'
op|'('
nl|'\n'
name|'_'
op|'('
string|'"Object reconstruction complete. (%.02f minutes)"'
op|')'
op|','
name|'total'
op|')'
newline|'\n'
name|'dump_recon_cache'
op|'('
op|'{'
string|"'object_reconstruction_time'"
op|':'
name|'total'
op|','
nl|'\n'
string|"'object_reconstruction_last'"
op|':'
name|'time'
op|'.'
name|'time'
op|'('
op|')'
op|'}'
op|','
nl|'\n'
name|'self'
op|'.'
name|'rcache'
op|','
name|'self'
op|'.'
name|'logger'
op|')'
newline|'\n'
name|'self'
op|'.'
name|'logger'
op|'.'
name|'debug'
op|'('
string|"'reconstruction sleeping for %s seconds.'"
op|','
nl|'\n'
name|'self'
op|'.'
name|'interval'
op|')'
newline|'\n'
name|'sleep'
op|'('
name|'self'
op|'.'
name|'interval'
op|')'
newline|'\n'
dedent|''
dedent|''
dedent|''
endmarker|''
end_unit
